{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Project 2 - Continuous Control\n",
    "### Udacity Deep Reinforcement Learning Nanodegree\n",
    "by Kitson Swann\n",
    "\n",
    "## Reacher Environment\n",
    "\n",
    "In this environment, our goal is to use a Deep Reinforcement Learning Agent to train a double-jointed arm to \n",
    "move its hand to a target location.\n",
    "\n",
    "For the purpose of this assignment, the task is considered solved when the agent achieves a score of +30 \n",
    "over 100 consecutive episodes.\n",
    "\n",
    "### States\n",
    "\n",
    "The state space contains 33 dimensions corresponding to: \n",
    "\n",
    "- position rotation  \n",
    "- velocity\n",
    "- angular velocities of the arm \n",
    "\n",
    "### Actions\n",
    "\n",
    "Each action is a vector with four numbers, corresponding to torque \n",
    "applicable to two joints. Every entry in the action vector should \n",
    "be a number between -1 and 1. \n",
    "\n",
    "### Rewards\n",
    "\n",
    "According to the project instructions a reward of +0.1 is provided for each step that the agent's hand \n",
    "is in the goal location. However, in practice the rewards from the environment are much more varied ranging from 0 to 0.5. I am curious if the environment was updated between the time this assignment was authored and now. The different reward structure did not prevent me from succeeding at the task, but I wonder if it made it more difficult.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Version 1\n",
    "\n",
    "For this project I had planned to initially begin with the simple single agent environment and move onto the more complex multi-agent environment after solving the simple one, but achieving the minimum average score on the single agent version proved quite difficult, so I stopped at solving just the simple environment. Below you can see the agent initialized with the state and action spaces shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import matplotlib.pyplot as plt\n",
    "from optimize import find_optimal_hyperparameters\n",
    "from train import train_ddpg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "env = UnityEnvironment(file_name='../env/v1/Reacher.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "As suggested in the project instructions I used the Deep Deterministic Policy Gradient agent from the Pendulum environment example [here](https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum) as a starting point. I modified the training loop to work with the Unity Reacher environment, which required some minor changes from the original which was based on an OpenAI Gym environment.\n",
    "\n",
    "### Initial Experimentation\n",
    "\n",
    "After getting the training routine working, I initially had no success in getting the agent to train with the standard hyper-parameters. The score was oscillating around 0-0.1 most of the time and not improving even over 100 episodes.\n",
    "\n",
    "Normally at this point I would have experimented with hyperparameter-optimization, but in this algorithm there are so many parameters that I felt the need to look for some good values. I consulted Udacity's Knowledge Base for some assistance, and saw a number of suggestions. I reviewed the suggestions and many repositories containing the solutions of other students for ideas. I made a number of changes as a result including:\n",
    "\n",
    "- Changing the training loop to end when done which for the Reacher environment is 1000 timesteps.\n",
    "- Tried different random seeds\n",
    "- Modified the architecture of the actor and critic neural networks to try different hidden layer sizes.\n",
    "- Modified the architecture of the actor and critic neural networks to add batch normalization.\n",
    "- Modified the ddpg agent to only do a learning step every 5, 10, 20, 40 iterations.\n",
    "- Reset the noise after every episode.\n",
    "- Tried different suggested values for buffer size, batch size, gamma, tau, weight decay and the sigma variable in the Ornstein-Uhlenbeck process noise implementation.\n",
    "\n",
    "Even with all of the above modifications I still wasn't seeing any decent results. The score rarely went above 0.5, and seemed to get worse over time.\n",
    "\n",
    "### Structured Hyper-parameter Optimization\n",
    "\n",
    "After seeing a number of people say the suggested changes should work, but having no success myself, I felt that I needed to take a more robust approach to hyper-parameter optimization. Similar to my implementation in Project 1, I used scikit-optimize's Gaussian Process Optimizer to search for a better set of hyper-parameters. Below is the search space that I initially defined.\n",
    "\n",
    "I initially did batches of 10 episodes to keep the run time short and look for some better initial values. Because the process takes a long time I watched the values from each run, and visually tried to understand what values of each parameter were working well. As I noticed good values, I iteratively removed parameters from the search space, instead setting them at reasonable levels until I found something that worked fairly well which I've called Reasonable Parameter Set below.\n",
    "\n",
    "I have included the call to the optimization routine below, but haven't run it as it takes up a lot of space, and I re-ran it iteratively so one single output was never the final one used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "space = [\n",
    "    Real(0, 0.5, \"uniform\", name='eps_end'),\n",
    "    Real(1e-5, 1e0, \"log-uniform\", name='eps_decay'),\n",
    "    Categorical([1e5, 1e6], name=\"buffer_size\"),\n",
    "    Categorical([64, 128, 256, 512], name=\"batch_size\"),\n",
    "    Categorical([1, 5, 10, 20, 40], name=\"update_every\"),\n",
    "    Categorical([1, 5, 10, 20], name=\"update_times\"),\n",
    "    Categorical([128, 256, 512], name=\"actor_fc1_units\"),\n",
    "    Categorical([128, 256, 512], name=\"actor_fc2_units\"),\n",
    "    Categorical([128, 256, 512], name=\"critic_fc1_units\"),\n",
    "    Categorical([128, 256, 512], name=\"critic_fc2_units\"),\n",
    "    Real(1e-5, 1e-3, \"log-uniform\", name='actor_lr'),\n",
    "    Real(1e-5, 1e-3, \"log-uniform\", name='critic_lr'),\n",
    "    Real(0.9, 0.99, \"uniform\", name=\"gamma\"),\n",
    "    Real(1e-2, 1e-1, \"log-uniform\", name=\"tau\"),\n",
    "    Real(0, 1e-3, \"uniform\", name=\"weight_decay\"),\n",
    "    Real(0.01, 0.05, \"log-uniform\", name=\"noise_theta\"),\n",
    "    Real(0.01, 0.05, \"log-uniform\", name=\"noise_sigma\"),\n",
    "]\n",
    "\n",
    "params, res_gp = find_optimal_hyperparameters(env=env, brain_name=brain_name, num_agents=num_agents, n_calls=50, episodes_per_batch=10, space=space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasonable Parameter Set\n",
    "\n",
    "This is the best resonable parameter set I found through a somewhat manual search for hyper-parameters. There are many parameters to tune here, and the search space grows so quickly that a full optimization run was going to take more time than I had available, so I viewed the results and manually tuned based on what I saw to develop the set of values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"random_seed\": 22,\n",
      "  \"noise_sigma\": 0.02,\n",
      "  \"noise_theta\": 0.015,\n",
      "  \"weight_decay\": 0,\n",
      "  \"tau\": 0.05,\n",
      "  \"gamma\": 0.95,\n",
      "  \"critic_lr\": 0.0001,\n",
      "  \"actor_lr\": 0.001,\n",
      "  \"critic_fc2_units\": 128,\n",
      "  \"critic_fc1_units\": 128,\n",
      "  \"actor_fc2_units\": 128,\n",
      "  \"actor_fc1_units\": 128,\n",
      "  \"update_times\": 10,\n",
      "  \"update_every\": 20,\n",
      "  \"batch_size\": 256,\n",
      "  \"buffer_size\": 1000000.0,\n",
      "  \"eps_decay\": 0.7,\n",
      "  \"eps_end\": 0.3,\n",
      "  \"eps_start\": 1.0,\n",
      "  \"action_size\": 4,\n",
      "  \"state_size\": 33,\n",
      "  \"n_episodes\": 2000,\n",
      "  \"solved_threshold\": 31.0,\n",
      "  \"break_early\": true,\n",
      "  \"name\": \"_optimal\",\n",
      "  \"num_agents\": 1,\n",
      "  \"brain_name\": \"ReacherBrain\"\n",
      "Replay Buffer Size: 1000000.0, Batch Size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kitson_swann/Documents/udacity_drl/udacity_drl_p2/src/ddpg_agent.py:140: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local.parameters(), 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 1.69\n",
      "Episode 200\tAverage Score: 7.48\n",
      "Episode 300\tAverage Score: 13.16\n",
      "Episode 400\tAverage Score: 16.45\n",
      "Episode 500\tAverage Score: 23.23\n",
      "Episode 600\tAverage Score: 29.82\n",
      "Episode 639\tAverage Score: 31.07\n",
      "Environment solved in 639 episodes!\tAverage Score: 31.07\n"
     ]
    }
   ],
   "source": [
    "optimal_params = {\n",
    "  \"random_seed\": 22,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 128,\n",
    "  \"critic_fc1_units\": 128,\n",
    "  \"actor_fc2_units\": 128,\n",
    "  \"actor_fc1_units\": 128,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 1000000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": 4,\n",
    "  \"state_size\": 33,\n",
    "  \"n_episodes\": 2000,\n",
    "  \"solved_threshold\": 31.0,\n",
    "  \"break_early\": True,\n",
    "  \"name\": \"_optimal\",\n",
    "  \"num_agents\": 1,\n",
    "  \"brain_name\": \"ReacherBrain\"\n",
    "}\n",
    "\n",
    "scores = train_ddpg(env=env, **optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"scores.json\", \"w\") as f:\n",
    "    json.dump({\"scores\": scores}, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Success !!!\n",
    "\n",
    "As you can see above the environment was solved in 639 episodes. In reality, it was a bit shorter than that, because I set the threshold to 31 just to go a bit beyond the minimim value.\n",
    "\n",
    "Below you can see the plots of raw scores and average scores that I took from Tensorboard.\n",
    "\n",
    "![tensorboard_raw_score.png](tensorboard_raw_score.png)\n",
    "![tensorboard_avg_score.png](tensorboard_avg_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch The Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env = UnityEnvironment(file_name='../env/v1/Reacher.app')\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "from ddpg_agent import Agent\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "params = {\n",
    "  \"random_seed\": 22,\n",
    "  \"noise_sigma\": 0.02,\n",
    "  \"noise_theta\": 0.015,\n",
    "  \"weight_decay\": 0,\n",
    "  \"tau\": 0.05,\n",
    "  \"gamma\": 0.95,\n",
    "  \"critic_lr\": 0.0001,\n",
    "  \"actor_lr\": 0.001,\n",
    "  \"critic_fc2_units\": 128,\n",
    "  \"critic_fc1_units\": 128,\n",
    "  \"actor_fc2_units\": 128,\n",
    "  \"actor_fc1_units\": 128,\n",
    "  \"update_times\": 10,\n",
    "  \"update_every\": 20,\n",
    "  \"batch_size\": 256,\n",
    "  \"buffer_size\": 1000000.0,\n",
    "  \"eps_decay\": 0.7,\n",
    "  \"eps_end\": 0.3,\n",
    "  \"eps_start\": 1.0,\n",
    "  \"action_size\": 4,\n",
    "  \"state_size\": 33\n",
    "}\n",
    "\n",
    "agent = Agent(**params)\n",
    "\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor_optimal.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic_optimal.pth'))\n",
    "        \n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "agent.reset()\n",
    "states = env_info.vector_observations\n",
    "agent_scores = np.zeros(1)\n",
    "while True:\n",
    "    actions = agent.act(state=states)\n",
    "    env_info = env.step(actions)[brain_name]\n",
    "    next_states = env_info.vector_observations\n",
    "    rewards = env_info.rewards\n",
    "    dones = env_info.local_done\n",
    "    if np.any(dones):\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
